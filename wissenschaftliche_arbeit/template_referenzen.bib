% =============================================================================
% Referenzen für Seminararbeit: Von Induction Heads zu Function Vectors
% Autor: Cabrell Valdice Teikeu Kana
% Stand: Dezember 2025
% =============================================================================

% -----------------------------------------------------------------------------
% KERNQUELLEN - Transformer Grundlagen
% -----------------------------------------------------------------------------

@inproceedings{VSP_17,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title     = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30 (NeurIPS)},
  year      = {2017},
  pages     = {5998--6008},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@article{ENO_21,
  author    = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  title     = {A Mathematical Framework for Transformer Circuits},
  journal   = {Transformer Circuits Thread},
  year      = {2021},
  url       = {https://transformer-circuits.pub/2021/framework/index.html}
}

% -----------------------------------------------------------------------------
% KERNQUELLEN - Induction Heads
% -----------------------------------------------------------------------------

@article{Ols_22,
  author    = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  title     = {In-context Learning and Induction Heads},
  journal   = {Transformer Circuits Thread},
  year      = {2022},
  url       = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{CHA_22,
  author    = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, James and Hill, Felix},
  title     = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  booktitle = {Advances in Neural Information Processing Systems 35 (NeurIPS)},
  year      = {2022},
  pages     = {18878--18891},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/77c6ccacfd9962e2307fc64680fc5ace-Abstract-Conference.html}
}

@article{STA_24,
  author    = {Edelman, Benjamin L. and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  title     = {The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},
  journal   = {arXiv preprint arXiv:2402.11004},
  year      = {2024},
  url       = {https://arxiv.org/abs/2402.11004}
}

@article{DAN_25,
  author    = {D'Angelo, Francesco and Croce, Francesco and Flammarion, Nicolas},
  title     = {Selective Induction Heads: How Transformers Select Causal Structures in Context},
  journal   = {arXiv preprint arXiv:2509.08184},
  year      = {2025},
  url       = {https://arxiv.org/abs/2509.08184}
}

% -----------------------------------------------------------------------------
% KERNQUELLEN - Function Vectors und Task Vectors
% -----------------------------------------------------------------------------

@inproceedings{TOD_24,
  author    = {Todd, Eric and Li, Millicent L. and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C. and Bau, David},
  title     = {Function Vectors in Large Language Models},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024},
  url       = {https://openreview.net/forum?id=AwyxtyMwaG}
}

@inproceedings{HEN_23,
  author    = {Hendel, Roee and Geva, Mor and Globerson, Amir},
  title     = {In-Context Learning Creates Task Vectors},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  year      = {2023},
  pages     = {9318--9333},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.624}
}

@article{YAN_25,
  author    = {Yang, Liu and Lin, Ziqian and Lee, Kangwook and Papailiopoulos, Dimitris and Nowak, Robert},
  title     = {Task Vectors in In-Context Learning: Emergence, Formation, and Benefit},
  journal   = {arXiv preprint arXiv:2501.09240},
  year      = {2025},
  url       = {https://arxiv.org/abs/2501.09240}
}

% -----------------------------------------------------------------------------
% KERNQUELLEN - Vergleich und aktuelle Forschung 2025
% -----------------------------------------------------------------------------

@inproceedings{YS_25,
  author    = {Yin, Kayo and Steinhardt, Jacob},
  title     = {Which Attention Heads Matter for In-Context Learning?},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2025},
  url       = {https://arxiv.org/abs/2502.14010}
}

@inproceedings{MIN_25,
  author    = {Minegishi, Gouki and Furuta, Hiroki and Taniguchi, Shohei and Iwasawa, Yusuke and Matsuo, Yutaka},
  title     = {Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2025},
  url       = {https://arxiv.org/abs/2505.16694}
}

@article{SAH_25,
  author    = {Sahin, Kerem and Feucht, Sheridan and Belfki, Adam and Brinkmann, Jannik and Mueller, Aaron and Bau, David and Wendler, Chris},
  title     = {In-Context Learning Without Copying},
  journal   = {arXiv preprint arXiv:2511.05743},
  year      = {2025},
  url       = {https://arxiv.org/abs/2511.05743}
}

@article{SIN_25,
  author    = {Singh, Aaditya K. and Moskovitz, Ted and Dragutinovic, Sara and Hill, Felix and Chan, Stephanie C. Y. and Saxe, Andrew M.},
  title     = {Strategy Coopetition Explains the Emergence and Transience of In-Context Learning},
  journal   = {arXiv preprint arXiv:2503.05631},
  year      = {2025},
  url       = {https://arxiv.org/abs/2503.05631}
}

@article{MUS_25,
  author    = {Musat, Tiberiu and Pimentel, Tiago and Noci, Lorenzo and Stolfo, Alessandro and Sachan, Mrinmaya and Hofmann, Thomas},
  title     = {On the Emergence of Induction Heads for In-Context Learning},
  journal   = {arXiv preprint arXiv:2511.01033},
  year      = {2025},
  url       = {https://arxiv.org/abs/2511.01033}
}

% -----------------------------------------------------------------------------
% ERGÄNZENDE QUELLEN - Mechanistic Interpretability
% -----------------------------------------------------------------------------

@article{OCS_20,
  author    = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title     = {Zoom In: An Introduction to Circuits},
  journal   = {Distill},
  year      = {2020},
  volume    = {5},
  number    = {3},
  url       = {https://distill.pub/2020/circuits/zoom-in/}
}

@article{WVC_22,
  author    = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  title     = {Interpretability in the Wild: A Circuit for Indirect Object Identification in {GPT-2} Small},
  journal   = {arXiv preprint arXiv:2211.00593},
  year      = {2022},
  url       = {https://arxiv.org/abs/2211.00593}
}

@article{CML_23,
  author    = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  title     = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  journal   = {Advances in Neural Information Processing Systems 36 (NeurIPS)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2304.14997}
}

% -----------------------------------------------------------------------------
% ERGÄNZENDE QUELLEN - In-Context Learning Theorie
% -----------------------------------------------------------------------------

@inproceedings{MIN_22,
  author    = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Arber, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  title     = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2022},
  pages     = {11048--11064},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.759}
}

@inproceedings{VON_23,
  author    = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  title     = {Transformers Learn In-Context by Gradient Descent},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2023},
  pages     = {35151--35174},
  url       = {https://proceedings.mlr.press/v202/von-oswald23a.html}
}

@inproceedings{AKY_23,
  author    = {Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  title     = {What learning algorithm is in-context learning? Investigations with linear models},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2023},
  url       = {https://openreview.net/forum?id=0g0X4H8yN4I}
}

% -----------------------------------------------------------------------------
% ERGÄNZENDE QUELLEN - Surveys und Überblicksarbeiten
% -----------------------------------------------------------------------------

@article{FSBC_24,
  author    = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R.},
  title     = {A Primer on the Inner Workings of Transformer-based Language Models},
  journal   = {arXiv preprint arXiv:2405.00208},
  year      = {2024},
  url       = {https://arxiv.org/abs/2405.00208}
}

@article{YANG_25b,
  author    = {Yang, Haolin and Cho, Hakaze and Inoue, Naoya},
  title     = {Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis},
  journal   = {arXiv preprint arXiv:2509.24164},
  year      = {2025},
  url       = {https://arxiv.org/abs/2509.24164}
}
